{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import load_rating_data as ld\n",
    "from utils import RMSE \n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim, nn\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CML(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        \n",
    "        super(CML, self).__init__()\n",
    "        self.args = args\n",
    "        self.n_users = args.n_users\n",
    "        self.n_items = args.n_items\n",
    "        self.latent_dim = args.latent_dim\n",
    "        self.margin = args.margin\n",
    "        self.lambda_c = args.lambda_c\n",
    "        self.neg_item_dic = {}\n",
    "        self.n_neg_samples = args.n_neg_samples\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(self.n_users, self.latent_dim, max_norm = 1) # restrict norms\n",
    "        self.item_embedding = nn.Embedding(self.n_items, self.latent_dim, max_norm = 1)\n",
    "        \n",
    "    \n",
    "    def distance_loss(self, i, j, k):\n",
    "        \"\"\"\n",
    "        compute distance loss\n",
    "        \"\"\"\n",
    "        user = self.user_embedding(i).view(len(i), 1, self.latent_dim) # (batch_size, 1, latent_dim)\n",
    "        item = self.item_embedding(j).view(len(j), 1, self.latent_dim) # (batch_size, 1, latent_dim)\n",
    "        neg_item = self.item_embedding(k)   # (batch_size, n_neg_samples, latent_dim)\n",
    "        d_ij = torch.cdist(user, item).view(-1, 1)**2 # (batch_size, 1)\n",
    "        d_ik = torch.cdist(user, neg_item).view(-1, self.n_neg_samples)**2  # (batch_size, n_neg_samples)\n",
    "        \n",
    "        metric = self.margin + d_ij - d_ik # (batch_size, n_neg_samples)\n",
    "        loss = 0\n",
    "        for i in range(len(metric)):\n",
    "            temp_metric = metric[i][metric[i]>0]    # []+\n",
    "            rank_d_ij = self.n_items * len(temp_metric) / self.n_neg_samples  # J x M / N\n",
    "            w_ij = np.log(rank_d_ij + 1)\n",
    "            loss +=  (w_ij * temp_metric).sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def cov_loss(self):\n",
    "        U = self.user_embedding(torch.LongTensor([x for x in range(self.n_users)]))\n",
    "        V = self.item_embedding(torch.LongTensor([x for x in range(self.n_items)]))\n",
    "        \n",
    "        matrix = torch.cat([U, V])\n",
    "        n_rows = matrix.shape[0]\n",
    "        matrix = matrix - torch.mean(matrix, dim=0)\n",
    "        cov = torch.matmul(matrix.T, matrix) / n_rows\n",
    "        loss = (torch.linalg.norm(cov) - torch.linalg.norm(torch.diagonal(cov),2))/self.n_users\n",
    "        \n",
    "        return loss * self.lambda_c\n",
    "    \n",
    "    \n",
    "    def create_train_dataset(self, train):\n",
    "        \n",
    "        pos_item_dic = {}\n",
    "        total_item = np.arange(self.n_items)\n",
    "        \n",
    "        neg_item = []\n",
    "        for row in train.itertuples():\n",
    "            if row.user_id not in pos_item_dic.keys():\n",
    "                mask = (train.user_id == row.user_id)\n",
    "                pos_item_dic[row.user_id] = np.array(train.loc[mask, :].item_id)\n",
    "                self.neg_item_dic[row.user_id] = np.setdiff1d(total_item, pos_item_dic[row.user_id])\n",
    "            neg = np.random.choice(self.neg_item_dic[row.user_id], self.n_neg_samples)\n",
    "            neg_item.append(neg)\n",
    "\n",
    "        dataset = RatingDataset(user_tensor = torch.LongTensor(train.iloc[:, 0]),\n",
    "                            item_tensor = torch.LongTensor(train.iloc[:, 1]),\n",
    "                            neg_item_list = torch.LongTensor(neg_item))\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    \n",
    "    def evaluate(self, train, test):\n",
    "        U = self.user_embedding(torch.LongTensor([x for x in range(self.n_users)]))\n",
    "        V = self.item_embedding(torch.LongTensor([x for x in range(self.n_items)]))\n",
    "        dist = torch.cdist(U, V)\n",
    "        \n",
    "        #for row in train.itertuples():\n",
    "        #    dist[int(row.user_id), int(row.item_id)] = 1000\n",
    "            \n",
    "        top50_id = torch.topk(dist, k=50, dim=1, largest=False)[1].numpy()\n",
    "        top100_id = torch.topk(dist, k=100, dim=1, largest=False)[1].numpy()\n",
    "        hit_50 = 0\n",
    "        hit_100 = 0\n",
    "        \n",
    "        for i in range(len(test)):\n",
    "            if int(test.iloc[i, 1]) in top50_id[int(test.iloc[i, 0])]:\n",
    "                hit_50 += 1\n",
    "            if int(test.iloc[i, 1]) in top100_id[int(test.iloc[i, 0])]:\n",
    "                hit_100 += 1\n",
    "        \n",
    "        r50, r100 =  hit_50/len(test), hit_100/len(test)\n",
    "        return r50, r100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, user_tensor, item_tensor, neg_item_list):\n",
    "        self.user_tensor = user_tensor\n",
    "        self.item_tensor = item_tensor\n",
    "        self.neg_items = neg_item_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.user_tensor[index], self.item_tensor[index], self.neg_items[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "args = edict()\n",
    "\n",
    "# training options\n",
    "args.latent_dim = 32\n",
    "args.margin = 0.5\n",
    "args.lambda_c = 10\n",
    "args.epoch = 10                      # training epoch.\n",
    "args.n_neg_samples = 10\n",
    "args.batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    \n",
    "    # Basic settings\n",
    "    from torch.utils.tensorboard import SummaryWriter \n",
    "    writer = SummaryWriter(\"cml_logs\")\n",
    "    \n",
    "    df = ld.load_rating_1m()\n",
    "    args.n_users, args.n_items = len(df.user_id.unique()), len(df.item_id.unique())\n",
    "    ratio = 0.8\n",
    "    train, test = train_test_split(df, test_size=1-ratio)\n",
    "    val, test = train_test_split(test, test_size=0.5)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    val.reset_index(drop=True, inplace=True)\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    model = CML(args) #.to(device)\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Start training\n",
    "    # Save the starting time\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(0, args.epoch):\n",
    "        # Here starts the train loop.\n",
    "        dataset = model.create_train_dataset(train)\n",
    "        train_loader = DataLoader(dataset, batch_size = args.batch_size, shuffle = True)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            user, item, neg_items = batch[0], batch[1], batch[2]\n",
    "            loss = model.distance_loss(user, item, neg_items) + model.cov_loss()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        t = time.time()-start_time\n",
    "        model.eval()\n",
    "        recall_50, recall_100 = model.evaluate(train, test)\n",
    "        writer.add_scalar(\"training_loss\", total_loss, epoch)\n",
    "        writer.add_scalar(\"recall@50\", recall_50, epoch)\n",
    "        writer.add_scalar(\"recall@100\", recall_100, epoch)\n",
    "        print(\"epoch = {:d}, total_loss = {:.4f}, recall@50 = {:.4f}, recall@100 = {:.4f}, epoch_time = {:.4f}sec\".format(epoch, total_loss, recall_50, recall_100, time.time()-start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, total_loss = 18903719.7617, recall@50 = 0.0558, recall@100 = 0.1150, epoch_time = 455.0343sec\n",
      "epoch = 1, total_loss = 15060878.7178, recall@50 = 0.0542, recall@100 = 0.1194, epoch_time = 409.5592sec\n",
      "epoch = 2, total_loss = 14171802.2627, recall@50 = 0.0511, recall@100 = 0.1179, epoch_time = 511.3636sec\n",
      "epoch = 3, total_loss = 13685609.4775, recall@50 = 0.0490, recall@100 = 0.1162, epoch_time = 445.2548sec\n"
     ]
    }
   ],
   "source": [
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
