{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import load_rating_data as ld\n",
    "import time\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(GMF, self).__init__() # run nn.Module.__init__()\n",
    "        self.args = args\n",
    "        self.n_users = args.n_users\n",
    "        self.n_items = args.n_items\n",
    "        self.latent_dim = args.latent_dim\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding_user = nn.Embedding(num_embeddings = self.n_users, embedding_dim = self.latent_dim)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings = self.n_items, embedding_dim = self.latent_dim)\n",
    "        \n",
    "        # One layer\n",
    "        self.affine_output = nn.Linear(in_features = self.latent_dim, out_features = 1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, u, i):\n",
    "        \n",
    "        user_embedding = self.embedding_user(u)\n",
    "        item_embedding = self.embedding_item(i)\n",
    "        product = torch.mul(user_embedding, item_embedding) # element-wise product\n",
    "        logits = self.affine_output(product)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(MLP, self).__init__()\n",
    "        self.args = args\n",
    "        self.n_users = args.n_users\n",
    "        self.n_items = args.n_items\n",
    "        self.latent_dim = args.latent_dim\n",
    "        \n",
    "        # Build Layers\n",
    "        # Embedding Layer\n",
    "        self.embedding_user = nn.Embedding(num_embeddings = self.n_users, embedding_dim = self.latent_dim)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings = self.n_items, embedding_dim = self.latent_dim)\n",
    "        \n",
    "        ## Fully Connected Layer\n",
    "        self.fc_layers = nn.ModuleList() # holds submodules in a list\n",
    "        for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])):\n",
    "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
    "        \n",
    "        ## Final Layer\n",
    "        self.affine_output = nn.Linear(in_features = args.layers[-1], out_features = 1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, u, i):\n",
    "        \n",
    "        user_embedding = self.embedding_user(u)\n",
    "        item_embedding = self.embedding_item(i)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1) # concatenate user, item\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            vector = self.fc_layers[idx](vector)\n",
    "            vector = nn.ReLU()(vector)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.args = args\n",
    "        self.n_users = args.n_users\n",
    "        self.n_items = args.n_items\n",
    "        self.latent_GMF = args.latent_GMF\n",
    "        self.latent_MLP = args.latent_MLP\n",
    "        \n",
    "        self.embedding_user_MF = nn.Embedding(num_embeddings = self.n_users, embedding_dim = self.latent_GMF)\n",
    "        self.embedding_item_MF = nn.Embedding(num_embeddings = self.n_items, embedding_dim = self.latent_GMF)\n",
    "        self.embedding_user_MLP = nn.Embedding(num_embeddings = self.n_users, embedding_dim = self.latent_MLP)\n",
    "        self.embedding_item_MLP = nn.Embedding(num_embeddings = self.n_items, embedding_dim = self.latent_MLP)\n",
    "        \n",
    "        ## Fully Connected Layer\n",
    "        self.fc_layers = nn.ModuleList() # holds submodules in a list\n",
    "        for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])):\n",
    "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
    "                        \n",
    "        self.affine_output = nn.Linear(in_features = args.layers[-1] + self.latent_GMF, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, u, i):\n",
    "        user_embedding_MF = self.embedding_user_MF(u)\n",
    "        item_embedding_MF = self.embedding_item_MF(i)\n",
    "        user_embedding_MLP = self.embedding_user_MLP(u)\n",
    "        item_embedding_MLP = self.embedding_item_MLP(i)\n",
    "        \n",
    "        # Multi-Layer Perceptron part\n",
    "        MLP_vector = torch.cat([user_embedding_MLP, item_embedding_MLP], dim=-1)\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            MLP_vector = self.fc_layers[idx](MLP_vector)\n",
    "            MLP_vector = nn.ReLU()(MLP_vector)\n",
    "        \n",
    "        # Martrix Factorization part\n",
    "        MF_vector = torch.mul(user_embedding_MF, item_embedding_MF)\n",
    "        \n",
    "        vector = torch.cat([MLP_vector, MF_vector], dim=-1)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    def load_pretrain_weights(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        load pretrained weights from GMF and MLP\n",
    "        \"\"\"\n",
    "        \n",
    "        mlp_model = MLP(self.args)\n",
    "        state_dict = torch.load('./checkpoints/MLP_Epoch10_HR0.1758_NDCG0.1736.model')\n",
    "        mlp_model.load_state_dict(state_dict)\n",
    "        self.embedding_user_MLP.weight.data = mlp_model.embedding_user.weight.data\n",
    "        self.embedding_item_MLP.weight.data = mlp_model.embedding_item.weight.data\n",
    "        for idx in range(len(self.fc_layers)):\n",
    "            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n",
    "        \n",
    "        gmf_model = GMF(args)\n",
    "        gmf_model\n",
    "        state_dict = torch.load('./checkpoints/GMF_Epoch10_HR0.1479_NDCG0.1472.model')\n",
    "        gmf_model.load_state_dict(state_dict)\n",
    "        self.embedding_user_MF.weight.data = gmf_model.embedding_user.weight.data\n",
    "        self.embedding_item_MF.weight.data = gmf_model.embedding_item.weight.data\n",
    "        \n",
    "        \n",
    "        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n",
    "        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "args = edict()\n",
    "\n",
    "# training options\n",
    "args.df = ld.load_rating_1m()\n",
    "args.latent_dim = 32\n",
    "args.epoch = 10                      # training epoch.\n",
    "args.train_n_neg = 3     #memory issue\n",
    "args.test_n_neg = 100     #memory issue\n",
    "args.batch_size = 256\n",
    "args.layers = [64,32,16]\n",
    "args.latent_GMF = 16\n",
    "args.latent_MLP = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "info = ld.information(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GMF(args)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evluating Epoch 1] total_loss = 1884.3949, HR = 0.0899, NDCG = 0.0736 time = 159.3931sec\n",
      "[Evluating Epoch 2] total_loss = 0.4397, HR = 0.1287, NDCG = 0.1271 time = 156.5461sec\n",
      "[Evluating Epoch 3] total_loss = 0.0001, HR = 0.1427, NDCG = 0.1419 time = 217.7750sec\n",
      "[Evluating Epoch 4] total_loss = 0.0000, HR = 0.1452, NDCG = 0.1444 time = 210.7375sec\n",
      "[Evluating Epoch 5] total_loss = 0.0000, HR = 0.1458, NDCG = 0.1450 time = 228.6337sec\n",
      "[Evluating Epoch 6] total_loss = 0.0000, HR = 0.1457, NDCG = 0.1450 time = 214.5030sec\n",
      "[Evluating Epoch 7] total_loss = 0.0000, HR = 0.1459, NDCG = 0.1452 time = 238.8860sec\n",
      "[Evluating Epoch 8] total_loss = 0.0000, HR = 0.1469, NDCG = 0.1462 time = 225.0136sec\n",
      "[Evluating Epoch 9] total_loss = 0.0000, HR = 0.1479, NDCG = 0.1472 time = 219.4985sec\n",
      "[Evluating Epoch 10] total_loss = 0.0000, HR = 0.1479, NDCG = 0.1472 time = 159.1954sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# epoch\n",
    "for epoch_id in range(1, args.epoch + 1):\n",
    "    train_loader = info.train_dataloader\n",
    "    start_epoch = timer()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        user, item, rating = batch[0], batch[1], batch[2]\n",
    "        # mini-batch update\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction.view(-1), rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    hit = 0\n",
    "    ndcg = 0\n",
    "    test_loader = info.test_dataloader\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        user, item, neg_items = batch[0], batch[1], batch[2]\n",
    "        # mini-batch update\n",
    "        test_pred = model(user, item)\n",
    "        neg_pred = model(user.reshape(-1, 1).expand(-1, args.test_n_neg), neg_items).view(-1, args.test_n_neg)\n",
    "        concat = torch.cat([test_pred, neg_pred], axis=1)\n",
    "        \n",
    "        _, indices = torch.topk(concat, 10)  # 10\n",
    "        indices = indices.numpy()\n",
    "        hit += (indices.size - np.count_nonzero(indices))\n",
    "        zero_idx = np.where(indices.reshape(-1) == 0)[0] % 10\n",
    "        ndcg += np.sum(np.reciprocal(np.log2(zero_idx + 2)))\n",
    "\n",
    "    hit_ratio = hit / len(info.test_dataset)\n",
    "    ndcg = ndcg / len(info.test_dataset)\n",
    "\n",
    "    print('[Evluating Epoch {}] total_loss = {:.4f}, HR = {:.4f}, NDCG = {:.4f} time = {:.4f}sec'.format(epoch_id, total_loss, hit_ratio, ndcg, timer()-start_epoch))\n",
    "    torch.save(model.state_dict(), \"checkpoints/GMF_Epoch{}_HR{:.4f}_NDCG{:.4f}.model\".format(epoch_id, hit_ratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(args)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evluating Epoch 1] total_loss = 1637.5050, HR = 0.1399, NDCG = 0.1206 time = 150.2823sec\n",
      "[Evluating Epoch 2] total_loss = 42.8607, HR = 0.1359, NDCG = 0.1263 time = 151.2220sec\n",
      "[Evluating Epoch 3] total_loss = 22.5133, HR = 0.1568, NDCG = 0.1490 time = 222.1022sec\n",
      "[Evluating Epoch 4] total_loss = 18.3688, HR = 0.1571, NDCG = 0.1514 time = 135.6831sec\n",
      "[Evluating Epoch 5] total_loss = 9.6168, HR = 0.1618, NDCG = 0.1576 time = 148.7477sec\n",
      "[Evluating Epoch 6] total_loss = 17.6119, HR = 0.1678, NDCG = 0.1636 time = 163.9576sec\n",
      "[Evluating Epoch 7] total_loss = 13.9895, HR = 0.1716, NDCG = 0.1688 time = 158.4741sec\n",
      "[Evluating Epoch 8] total_loss = 5.6223, HR = 0.1736, NDCG = 0.1711 time = 167.8705sec\n",
      "[Evluating Epoch 9] total_loss = 5.4288, HR = 0.1772, NDCG = 0.1748 time = 170.5959sec\n",
      "[Evluating Epoch 10] total_loss = 5.9292, HR = 0.1758, NDCG = 0.1736 time = 176.3812sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# epoch\n",
    "for epoch_id in range(1, args.epoch + 1):\n",
    "    train_loader = info.train_dataloader\n",
    "    start_epoch = timer()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        user, item, rating = batch[0], batch[1], batch[2]\n",
    "        # mini-batch update\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction.view(-1), rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    hit = 0\n",
    "    ndcg = 0\n",
    "    test_loader = info.test_dataloader\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        user, item, neg_items = batch[0], batch[1], batch[2]\n",
    "        # mini-batch update\n",
    "        test_pred = model(user, item)\n",
    "        neg_pred = model(user.reshape(-1, 1).expand(-1, args.test_n_neg), neg_items).view(-1, args.test_n_neg)\n",
    "        concat = torch.cat([test_pred, neg_pred], axis=1)\n",
    "        \n",
    "        _, indices = torch.topk(concat, 10)  # 10\n",
    "        indices = indices.numpy()\n",
    "        hit += (indices.size - np.count_nonzero(indices))\n",
    "        zero_idx = np.where(indices.reshape(-1) == 0)[0] % 10\n",
    "        ndcg += np.sum(np.reciprocal(np.log2(zero_idx + 2)))\n",
    "\n",
    "    hit_ratio = hit / len(info.test_dataset)\n",
    "    ndcg = ndcg / len(info.test_dataset)\n",
    "\n",
    "    print('[Evluating Epoch {}] total_loss = {:.4f}, HR = {:.4f}, NDCG = {:.4f} time = {:.4f}sec'.format(epoch_id, total_loss, hit_ratio, ndcg, timer()-start_epoch))\n",
    "    torch.save(model.state_dict(), \"checkpoints/MLP_Epoch{}_HR{:.4f}_NDCG{:.4f}.model\".format(epoch_id, hit_ratio, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuMF(args)\n",
    "model.load_pretrain_weights()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\KAIST\\KAIST\\개별연구\\논문구현\\NCF.ipynb 셀 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/KAIST/KAIST/%EA%B0%9C%EB%B3%84%EC%97%B0%EA%B5%AC/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/NCF.ipynb#ch0000015?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/KAIST/KAIST/%EA%B0%9C%EB%B3%84%EC%97%B0%EA%B5%AC/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/NCF.ipynb#ch0000015?line=5'>6</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/KAIST/KAIST/%EA%B0%9C%EB%B3%84%EC%97%B0%EA%B5%AC/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/NCF.ipynb#ch0000015?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/KAIST/KAIST/%EA%B0%9C%EB%B3%84%EC%97%B0%EA%B5%AC/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/NCF.ipynb#ch0000015?line=7'>8</a>\u001b[0m     user, item, rating \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m], batch[\u001b[39m1\u001b[39m], batch[\u001b[39m2\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/KAIST/KAIST/%EA%B0%9C%EB%B3%84%EC%97%B0%EA%B5%AC/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/NCF.ipynb#ch0000015?line=8'>9</a>\u001b[0m     \u001b[39m# mini-batch update\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:47\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 47\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:84\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     83\u001b[0m     transposed \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m     86\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meach element in list of batch should be of equal size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     83\u001b[0m     transposed \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[0;32m     86\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:56\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel)\n\u001b[0;32m     55\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[0;32m     57\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m     58\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     60\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# epoch\n",
    "for epoch_id in range(1, args.epoch + 1):\n",
    "    train_loader = info.train_dataloader\n",
    "    start_epoch = timer()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        user, item, rating = batch[0], batch[1], batch[2]\n",
    "        # mini-batch update\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction.view(-1), rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    hit = 0\n",
    "    ndcg = 0\n",
    "    test_loader = info.test_dataloader\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        user, item, neg_items = batch[0], batch[1], batch[2]\n",
    "        # mini-batch update\n",
    "        test_pred = model(user, item)\n",
    "        neg_pred = model(user.reshape(-1, 1).expand(-1, args.test_n_neg), neg_items).view(-1, args.test_n_neg)\n",
    "        concat = torch.cat([test_pred, neg_pred], axis=1)\n",
    "        \n",
    "        _, indices = torch.topk(concat, 10)  # 10\n",
    "        indices = indices.numpy()\n",
    "        hit += (indices.size - np.count_nonzero(indices))\n",
    "        zero_idx = np.where(indices.reshape(-1) == 0)[0] % 10\n",
    "        ndcg += np.sum(np.reciprocal(np.log2(zero_idx + 2)))\n",
    "\n",
    "    hit_ratio = hit / len(info.test_dataset)\n",
    "    ndcg = ndcg / len(info.test_dataset)\n",
    "\n",
    "    print('[Evluating Epoch {}] total_loss = {:.4f}, HR = {:.4f}, NDCG = {:.4f} time = {:.4f}sec'.format(epoch_id, total_loss, hit_ratio, ndcg, timer()-start_epoch))\n",
    "    torch.save(model.state_dict(), \"checkpoints/NeuMF_Epoch{}_HR{:.4f}_NDCG{:.4f}.model\".format(epoch_id, hit_ratio, ndcg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
