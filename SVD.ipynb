{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import load_data as ld\n",
    "from utils import RMSE \n",
    "import copy\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD():\n",
    "    \n",
    "    def __init__(self, R, val_R, latent_size=50, ld=1e-3, learning_rate=0.001, epochs=200):\n",
    "        \n",
    "        self._R = R             # explicit matrix\n",
    "        self._val_R = val_R\n",
    "        self._I = copy.deepcopy(self._R)    # implicit matrix\n",
    "        self._I[self._I != 0] = 1\n",
    "        self._val_I = copy.deepcopy(self._val_R)    # implicit matrix\n",
    "        self._val_I[self._val_I != 0] = 1\n",
    "        self._N, self._M = R.shape\n",
    "        self._latent = latent_size\n",
    "        self._lr = learning_rate\n",
    "        self._epochs =epochs\n",
    "        self._lambda = ld        \n",
    "        self._P = np.random.normal(0, 0.1, size=(self._N, latent_size))\n",
    "        self._Q = np.random.normal(0, 0.1, size=(self._M, latent_size))\n",
    "        self.b_u = np.zeros(self._N)\n",
    "        self.b_i = np.zeros(self._M)\n",
    "        self.mu = np.mean(self._R[np.where(self._R != 0)])\n",
    "\n",
    "                    \n",
    "    def update_SVD(self):\n",
    "        loss = self._I* (self._R - self.get_pred())\n",
    "        # derivate of U\n",
    "        grads_p = np.dot(loss, -self._Q) + self._lambda*self._P\n",
    "        # derivate of V\n",
    "        grads_q = np.dot(loss.T, -self._P) + self._lambda*self._Q   \n",
    "        \n",
    "        self.b_u = self.b_u - self._lr * (-np.mean(loss, axis=1) + self._lambda * self.b_u) \n",
    "        self.b_i = self.b_i - self._lr * (-np.mean(loss, axis=0) + self._lambda * self.b_i)        \n",
    "        self._P = self._P - self._lr * grads_p\n",
    "        self._Q = self._Q - self._lr * grads_q\n",
    "    \n",
    "    def get_pred(self):\n",
    "        return self.mu + self.b_u[:, np.newaxis] + self.b_i[np.newaxis,:] + np.dot(self._P, self._Q.T)\n",
    "            \n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        train_rmse_list = []\n",
    "        vali_rmse_list = []\n",
    "        last_vali_rmse = None\n",
    "        \n",
    "        start_time = timer()\n",
    "        for epoch in range(self._epochs):\n",
    "            start = timer()\n",
    "            \n",
    "            self.update_SVD()\n",
    "            pred = self.get_pred()\n",
    "            train_rmse = RMSE(self._R, pred)\n",
    "            val_rmse =  RMSE(self._val_R, pred)\n",
    "                \n",
    "            train_rmse_list.append(train_rmse)\n",
    "            vali_rmse_list.append(val_rmse)\n",
    "            \n",
    "            print('traning iteration:{: d} ,train_RMSE:{: f}, val_RMSE:{: f}'.format(epoch, train_rmse, val_rmse))\n",
    "            \n",
    "            if last_vali_rmse and last_vali_rmse - val_rmse <= -0.0001:\n",
    "                print('convergence at iterations:{: d}'.format(epoch))\n",
    "                break\n",
    "            else:\n",
    "                last_vali_rmse = val_rmse\n",
    "            \n",
    "                \n",
    "        print(\"Total time for training : %.4f\" % (timer()-start_time))\n",
    "        return self._P, self._Q, train_rmse_list, vali_rmse_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N, M :  943 1682\n"
     ]
    }
   ],
   "source": [
    "df = ld.load_rating_data()\n",
    "N, M = len(df.user_id.unique()), len(df.item_id.unique())\n",
    "ratio = 0.8\n",
    "print(\"N, M : \", N, M)\n",
    "train, test = train_test_split(df, test_size=1-ratio)\n",
    "val, test = train_test_split(test, test_size=0.5)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.zeros([N, M])    \n",
    "for i in range(len(train)):\n",
    "    R[int(train.iloc[i, 0])-1, int(train.iloc[i, 1])-1] = float(train.iloc[i, 2])\n",
    "\n",
    "val_R = np.zeros([N, M])    \n",
    "for i in range(len(val)):\n",
    "    val_R[int(val.iloc[i, 0])-1, int(val.iloc[i, 1])-1] = float(val.iloc[i, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 0 ,train_RMSE: 1.125899, val_RMSE: 1.129653\n",
      "traning iteration: 1 ,train_RMSE: 1.123542, val_RMSE: 1.129468\n",
      "traning iteration: 2 ,train_RMSE: 1.121189, val_RMSE: 1.129277\n",
      "traning iteration: 3 ,train_RMSE: 1.118810, val_RMSE: 1.129066\n",
      "traning iteration: 4 ,train_RMSE: 1.116375, val_RMSE: 1.128821\n",
      "traning iteration: 5 ,train_RMSE: 1.113853, val_RMSE: 1.128525\n",
      "traning iteration: 6 ,train_RMSE: 1.111209, val_RMSE: 1.128161\n",
      "traning iteration: 7 ,train_RMSE: 1.108404, val_RMSE: 1.127706\n",
      "traning iteration: 8 ,train_RMSE: 1.105393, val_RMSE: 1.127133\n",
      "traning iteration: 9 ,train_RMSE: 1.102125, val_RMSE: 1.126409\n",
      "traning iteration: 10 ,train_RMSE: 1.098540, val_RMSE: 1.125495\n",
      "traning iteration: 11 ,train_RMSE: 1.094570, val_RMSE: 1.124345\n",
      "traning iteration: 12 ,train_RMSE: 1.090137, val_RMSE: 1.122904\n",
      "traning iteration: 13 ,train_RMSE: 1.085155, val_RMSE: 1.121112\n",
      "traning iteration: 14 ,train_RMSE: 1.079530, val_RMSE: 1.118901\n",
      "traning iteration: 15 ,train_RMSE: 1.073165, val_RMSE: 1.116202\n",
      "traning iteration: 16 ,train_RMSE: 1.065968, val_RMSE: 1.112952\n",
      "traning iteration: 17 ,train_RMSE: 1.057864, val_RMSE: 1.109099\n",
      "traning iteration: 18 ,train_RMSE: 1.048803, val_RMSE: 1.104616\n",
      "traning iteration: 19 ,train_RMSE: 1.038784, val_RMSE: 1.099514\n",
      "traning iteration: 20 ,train_RMSE: 1.027858, val_RMSE: 1.093849\n",
      "traning iteration: 21 ,train_RMSE: 1.016142, val_RMSE: 1.087726\n",
      "traning iteration: 22 ,train_RMSE: 1.003808, val_RMSE: 1.081294\n",
      "traning iteration: 23 ,train_RMSE: 0.991063, val_RMSE: 1.074722\n",
      "traning iteration: 24 ,train_RMSE: 0.978126, val_RMSE: 1.068176\n",
      "traning iteration: 25 ,train_RMSE: 0.965191, val_RMSE: 1.061796\n",
      "traning iteration: 26 ,train_RMSE: 0.952414, val_RMSE: 1.055682\n",
      "traning iteration: 27 ,train_RMSE: 0.939904, val_RMSE: 1.049892\n",
      "traning iteration: 28 ,train_RMSE: 0.927729, val_RMSE: 1.044454\n",
      "traning iteration: 29 ,train_RMSE: 0.915927, val_RMSE: 1.039377\n",
      "traning iteration: 30 ,train_RMSE: 0.904513, val_RMSE: 1.034661\n",
      "traning iteration: 31 ,train_RMSE: 0.893488, val_RMSE: 1.030297\n",
      "traning iteration: 32 ,train_RMSE: 0.882839, val_RMSE: 1.026278\n",
      "traning iteration: 33 ,train_RMSE: 0.872547, val_RMSE: 1.022590\n",
      "traning iteration: 34 ,train_RMSE: 0.862587, val_RMSE: 1.019219\n",
      "traning iteration: 35 ,train_RMSE: 0.852931, val_RMSE: 1.016148\n",
      "traning iteration: 36 ,train_RMSE: 0.843550, val_RMSE: 1.013362\n",
      "traning iteration: 37 ,train_RMSE: 0.834419, val_RMSE: 1.010842\n",
      "traning iteration: 38 ,train_RMSE: 0.825512, val_RMSE: 1.008572\n",
      "traning iteration: 39 ,train_RMSE: 0.816808, val_RMSE: 1.006537\n",
      "traning iteration: 40 ,train_RMSE: 0.808290, val_RMSE: 1.004722\n",
      "traning iteration: 41 ,train_RMSE: 0.799942, val_RMSE: 1.003115\n",
      "traning iteration: 42 ,train_RMSE: 0.791752, val_RMSE: 1.001703\n",
      "traning iteration: 43 ,train_RMSE: 0.783708, val_RMSE: 1.000475\n",
      "traning iteration: 44 ,train_RMSE: 0.775802, val_RMSE: 0.999422\n",
      "traning iteration: 45 ,train_RMSE: 0.768026, val_RMSE: 0.998534\n",
      "traning iteration: 46 ,train_RMSE: 0.760376, val_RMSE: 0.997802\n",
      "traning iteration: 47 ,train_RMSE: 0.752846, val_RMSE: 0.997218\n",
      "traning iteration: 48 ,train_RMSE: 0.745432, val_RMSE: 0.996774\n",
      "traning iteration: 49 ,train_RMSE: 0.738132, val_RMSE: 0.996464\n",
      "traning iteration: 50 ,train_RMSE: 0.730942, val_RMSE: 0.996280\n",
      "traning iteration: 51 ,train_RMSE: 0.723861, val_RMSE: 0.996216\n",
      "traning iteration: 52 ,train_RMSE: 0.716886, val_RMSE: 0.996265\n",
      "traning iteration: 53 ,train_RMSE: 0.710018, val_RMSE: 0.996421\n",
      "convergence at iterations: 53\n",
      "Total time for training : 13.0347\n"
     ]
    }
   ],
   "source": [
    "svd = SVD(R, val_R, latent_size=50, ld=0.01, learning_rate=0.002, epochs=100)\n",
    "P1, Q1, train_rmse_list, vali_rmse_list = svd.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ea2312a96e00fc4424234a471f608c001e599e03afda339b55d660fc50d3e28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
