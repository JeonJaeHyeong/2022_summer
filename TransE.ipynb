{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from load_fb15k import TrainSet, TestSet\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TranE(nn.Module):\n",
    "    def __init__(self, entity_num, relation_num, device, dim=50, d_norm=2, gamma=1):\n",
    "        \"\"\"\n",
    "        :param entity_num: number of entities\n",
    "        :param relation_num: number of relations\n",
    "        :param dim: embedding dim\n",
    "        :param device:\n",
    "        :param d_norm: measure d(h+l, t), either L1-norm or L2-norm\n",
    "        :param gamma: margin hyperparameter\n",
    "        \"\"\"\n",
    "        super(TranE, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.d_norm = d_norm\n",
    "        self.device = device\n",
    "        self.gamma = torch.FloatTensor([gamma]).to(self.device)\n",
    "        self.entity_num = entity_num\n",
    "        self.relation_num = relation_num\n",
    "        self.entity_embedding = nn.Embedding.from_pretrained(\n",
    "            torch.empty(entity_num, self.dim).uniform_(-6 / math.sqrt(self.dim), 6 / math.sqrt(self.dim)), freeze=False)\n",
    "        self.relation_embedding = nn.Embedding.from_pretrained(\n",
    "            torch.empty(relation_num, self.dim).uniform_(-6 / math.sqrt(self.dim), 6 / math.sqrt(self.dim)),\n",
    "            freeze=False)\n",
    "        # l <= l / ||l||\n",
    "        relation_norm = torch.norm(self.relation_embedding.weight.data, dim=1, keepdim=True)\n",
    "        self.relation_embedding.weight.data = self.relation_embedding.weight.data / relation_norm\n",
    "\n",
    "    def forward(self, pos_head, pos_relation, pos_tail, neg_head, neg_relation, neg_tail):\n",
    "        \"\"\"\n",
    "        :param pos_head: [batch_size]\n",
    "        :param pos_relation: [batch_size]\n",
    "        :param pos_tail: [batch_size]\n",
    "        :param neg_head: [batch_size]\n",
    "        :param neg_relation: [batch_size]\n",
    "        :param neg_tail: [batch_size]\n",
    "        :return: triples loss\n",
    "        \"\"\"\n",
    "        pos_dis = self.entity_embedding(pos_head) + self.relation_embedding(pos_relation) - self.entity_embedding(\n",
    "            pos_tail)\n",
    "        neg_dis = self.entity_embedding(neg_head) + self.relation_embedding(neg_relation) - self.entity_embedding(\n",
    "            neg_tail)\n",
    "        # return pos_head_and_relation, pos_tail, neg_head_and_relation, neg_tail\n",
    "        return self.calculate_loss(pos_dis, neg_dis).requires_grad_()\n",
    "\n",
    "    def calculate_loss(self, pos_dis, neg_dis):\n",
    "        \"\"\"\n",
    "        :param pos_dis: [batch_size, embed_dim]\n",
    "        :param neg_dis: [batch_size, embed_dim]\n",
    "        :return: triples loss: [batch_size]\n",
    "        \"\"\"\n",
    "        distance_diff = self.gamma + torch.norm(pos_dis, p=self.d_norm, dim=1) - torch.norm(neg_dis, p=self.d_norm,\n",
    "                                                                                            dim=1)\n",
    "        return torch.sum(F.relu(distance_diff))\n",
    "\n",
    "    def tail_predict(self, head, relation, tail, k=10):\n",
    "        \"\"\"\n",
    "        to do tail prediction hits@k\n",
    "        :param head: [batch_size]\n",
    "        :param relation: [batch_size]\n",
    "        :param tail: [batch_size]\n",
    "        :param k: hits@k\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # head: [batch_size]\n",
    "        # h_and_r: [batch_size, embed_size] => [batch_size, 1, embed_size] => [batch_size, N, embed_size]\n",
    "        h_and_r = self.entity_embedding(head) + self.relation_embedding(relation)\n",
    "        h_and_r = torch.unsqueeze(h_and_r, dim=1)\n",
    "        h_and_r = h_and_r.expand(h_and_r.shape[0], self.entity_num, self.dim)\n",
    "        # embed_tail: [batch_size, N, embed_size]\n",
    "        embed_tail = self.entity_embedding.weight.data.expand(h_and_r.shape[0], self.entity_num, self.dim)\n",
    "        # indices: [batch_size, k]\n",
    "        values, indices = torch.topk(torch.norm(h_and_r - embed_tail, dim=2), k, dim=1, largest=False)\n",
    "        # tail: [batch_size] => [batch_size, 1]\n",
    "        tail = tail.view(-1, 1)\n",
    "        return torch.sum(torch.eq(indices, tail)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "embed_dim = 50\n",
    "num_epochs = 50\n",
    "train_batch_size = 32\n",
    "test_batch_size = 256\n",
    "lr = 1e-2\n",
    "momentum = 0\n",
    "gamma = 1\n",
    "d_norm = 2\n",
    "top_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 14951 entities, 1345 relations, 483142 triplets.\n",
      "Test set: 59071 triplets\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrainSet()\n",
    "test_dataset = TestSet()\n",
    "test_dataset.convert_word_to_index(train_dataset.entity_to_index, train_dataset.relation_to_index,\n",
    "                                       test_dataset.raw_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "transe = TranE(train_dataset.entity_num, train_dataset.relation_num, device, dim=embed_dim, d_norm=d_norm,\n",
    "                gamma=gamma).to(device)\n",
    "optimizer = optim.SGD(transe.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss = 0.8006324084371367\n",
      "===>epoch 1, test accuracy 0.21287941629564422\n",
      "epoch 2, loss = 0.6110726407517862\n",
      "===>epoch 2, test accuracy 0.24372365458516024\n",
      "epoch 3, loss = 0.465637634008911\n",
      "===>epoch 3, test accuracy 0.2644783396251968\n",
      "epoch 4, loss = 0.35256436182743967\n",
      "===>epoch 4, test accuracy 0.28269370757224355\n",
      "epoch 5, loss = 0.27879381454206437\n",
      "===>epoch 5, test accuracy 0.29620287450694927\n",
      "epoch 6, loss = 0.2320634035086554\n",
      "===>epoch 6, test accuracy 0.3076298014254033\n",
      "epoch 7, loss = 0.20112843379949455\n",
      "===>epoch 7, test accuracy 0.31382573513229844\n",
      "epoch 8, loss = 0.1795059778154809\n",
      "===>epoch 8, test accuracy 0.3217653332430465\n",
      "epoch 9, loss = 0.16349206028165192\n",
      "===>epoch 9, test accuracy 0.3250156591220734\n",
      "epoch 10, loss = 0.15096781277536048\n",
      "===>epoch 10, test accuracy 0.3303990113592118\n",
      "epoch 11, loss = 0.14093025786640273\n",
      "===>epoch 11, test accuracy 0.33386941138629783\n",
      "epoch 12, loss = 0.13270950060343936\n",
      "===>epoch 12, test accuracy 0.33688273433664573\n",
      "epoch 13, loss = 0.12588157593582416\n",
      "===>epoch 13, test accuracy 0.3387279714242183\n",
      "epoch 14, loss = 0.12012876147038964\n",
      "===>epoch 14, test accuracy 0.33908347581723686\n",
      "epoch 15, loss = 0.11526706449187189\n",
      "===>epoch 15, test accuracy 0.34103028558852905\n",
      "epoch 16, loss = 0.11111802529339941\n",
      "===>epoch 16, test accuracy 0.3424861607218432\n",
      "epoch 17, loss = 0.10752792792799692\n",
      "===>epoch 17, test accuracy 0.34223222901254424\n",
      "epoch 18, loss = 0.10438666771591151\n",
      "===>epoch 18, test accuracy 0.3432140982885003\n",
      "epoch 19, loss = 0.10167021931221225\n",
      "===>epoch 19, test accuracy 0.3430786680435408\n",
      "epoch 20, loss = 0.09925682969227666\n",
      "===>epoch 20, test accuracy 0.34455147195747493\n",
      "epoch 21, loss = 0.09713966345509814\n",
      "===>epoch 21, test accuracy 0.344026679758257\n",
      "epoch 22, loss = 0.09519944324375211\n",
      "===>epoch 22, test accuracy 0.34578727294272993\n",
      "epoch 23, loss = 0.09353296295127975\n",
      "===>epoch 23, test accuracy 0.3439589646357773\n",
      "epoch 24, loss = 0.09198670206710438\n",
      "===>epoch 24, test accuracy 0.3434003148753195\n",
      "epoch 25, loss = 0.09059337618534348\n",
      "===>epoch 25, test accuracy 0.3447207597636742\n",
      "epoch 26, loss = 0.08934862692481028\n",
      "===>epoch 26, test accuracy 0.34593963196830935\n",
      "epoch 27, loss = 0.08820064951020484\n",
      "===>epoch 27, test accuracy 0.34692150124426535\n",
      "epoch 28, loss = 0.08714232150254736\n",
      "===>epoch 28, test accuracy 0.34524555196289214\n",
      "epoch 29, loss = 0.08619099886228161\n",
      "===>epoch 29, test accuracy 0.3450085490342131\n",
      "epoch 30, loss = 0.08532424562238564\n",
      "===>epoch 30, test accuracy 0.34663371197372655\n",
      "epoch 31, loss = 0.0844710444632603\n",
      "===>epoch 31, test accuracy 0.3465321392900069\n",
      "epoch 32, loss = 0.08374971033374354\n",
      "===>epoch 32, test accuracy 0.3452116944016522\n",
      "epoch 33, loss = 0.08304501513637043\n",
      "===>epoch 33, test accuracy 0.3434511012171793\n",
      "epoch 34, loss = 0.08237753548985961\n",
      "===>epoch 34, test accuracy 0.346024275871409\n",
      "epoch 35, loss = 0.08175604629289406\n",
      "===>epoch 35, test accuracy 0.3462104924582282\n",
      "epoch 36, loss = 0.08120474133136767\n",
      "===>epoch 36, test accuracy 0.34522862318227215\n",
      "epoch 37, loss = 0.08066926694709928\n",
      "===>epoch 37, test accuracy 0.34492390513111343\n",
      "epoch 38, loss = 0.08016432706320631\n",
      "===>epoch 38, test accuracy 0.34443297049313537\n",
      "epoch 39, loss = 0.0797087866073734\n",
      "===>epoch 39, test accuracy 0.3452963383047519\n",
      "epoch 40, loss = 0.07927038856233294\n",
      "===>epoch 40, test accuracy 0.34504240659545293\n",
      "epoch 41, loss = 0.07884893594131073\n",
      "===>epoch 41, test accuracy 0.3445853295187148\n",
      "epoch 42, loss = 0.0784829095736394\n",
      "===>epoch 42, test accuracy 0.34573648660087014\n",
      "epoch 43, loss = 0.078109811934318\n",
      "===>epoch 43, test accuracy 0.3443483265900357\n",
      "epoch 44, loss = 0.0777392901692423\n",
      "===>epoch 44, test accuracy 0.3449577626923533\n",
      "epoch 45, loss = 0.07743183591445155\n",
      "===>epoch 45, test accuracy 0.34297709535982124\n",
      "epoch 46, loss = 0.07709839955270141\n",
      "===>epoch 46, test accuracy 0.34453454317685495\n",
      "epoch 47, loss = 0.07679548833059625\n",
      "===>epoch 47, test accuracy 0.3432140982885003\n",
      "epoch 48, loss = 0.07652359494089814\n",
      "===>epoch 48, test accuracy 0.34341724365593945\n",
      "epoch 49, loss = 0.07624830572256473\n",
      "===>epoch 49, test accuracy 0.34336645731407966\n",
      "epoch 50, loss = 0.07602633184341102\n",
      "===>epoch 50, test accuracy 0.34262159096680267\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # e <= e / ||e||\n",
    "    entity_norm = torch.norm(transe.entity_embedding.weight.data, dim=1, keepdim=True)\n",
    "    transe.entity_embedding.weight.data = transe.entity_embedding.weight.data / entity_norm\n",
    "    total_loss = 0\n",
    "    for batch_idx, (pos, neg) in enumerate(train_loader):\n",
    "        pos, neg = pos.to(device), neg.to(device)\n",
    "        # pos: [batch_size, 3] => [3, batch_size]\n",
    "        pos = torch.transpose(pos, 0, 1)\n",
    "        # pos_head, pos_relation, pos_tail: [batch_size]\n",
    "        pos_head, pos_relation, pos_tail = pos[0], pos[1], pos[2]\n",
    "        neg = torch.transpose(neg, 0, 1)\n",
    "        # neg_head, neg_relation, neg_tail: [batch_size]\n",
    "        neg_head, neg_relation, neg_tail = neg[0], neg[1], neg[2]\n",
    "        loss = transe(pos_head, pos_relation, pos_tail, neg_head, neg_relation, neg_tail)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch {epoch+1}, loss = {total_loss/train_dataset.__len__()}\")\n",
    "    corrct_test = 0\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        # data: [batch_size, 3] => [3, batch_size]\n",
    "        data = torch.transpose(data, 0, 1)\n",
    "        corrct_test += transe.tail_predict(data[0], data[1], data[2], k=top_k)\n",
    "    print(f\"===>epoch {epoch+1}, test accuracy {corrct_test/test_dataset.__len__()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
